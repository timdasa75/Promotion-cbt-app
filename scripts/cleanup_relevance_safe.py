#!/usr/bin/env python3
"""
Safe relevance cleanup pass.

Rule:
- Remove a high-confidence misplaced question ONLY if an equivalent
  normalized question already exists in the suggested target topic.

This avoids deleting unique content and focuses on clear cross-topic leakage.

Inputs:
- docs/question_quality_audit.json (generated by audit_question_quality.py)

Outputs:
- docs/relevance_cleanup_proposed_removals.json
- docs/relevance_cleanup_proposed_removals.md
"""

from __future__ import annotations

import argparse
import json
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple


ROOT = Path(__file__).resolve().parents[1]
AUDIT_JSON = ROOT / "docs" / "question_quality_audit.json"
TOPICS_FILE = ROOT / "data" / "topics.json"
DEFAULT_JSON_OUT = ROOT / "docs" / "relevance_cleanup_proposed_removals.json"
DEFAULT_MD_OUT = ROOT / "docs" / "relevance_cleanup_proposed_removals.md"


def load_json(path: Path):
    with path.open(encoding="utf-8") as f:
        return json.load(f)


def dump_json(path: Path, data):
    path.write_text(json.dumps(data, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")


def normalize_text(text: str) -> str:
    text = (text or "").lower()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def collect_topic_files():
    topics_doc = load_json(TOPICS_FILE)
    topics = topics_doc.get("topics", [])
    out = []
    for topic in topics:
        if not isinstance(topic, dict):
            continue
        rel_file = topic.get("file")
        if not rel_file:
            continue
        out.append(
            {
                "topic_id": topic.get("id", ""),
                "topic_name": topic.get("name", ""),
                "file": rel_file,
            }
        )
    return out


def iterate_subcategory_questions(subcategory: dict) -> Tuple[List[dict], bool]:
    questions = subcategory.get("questions")
    if not isinstance(questions, list):
        return [], False

    sub_id = subcategory.get("id")
    if (
        questions
        and isinstance(questions[0], dict)
        and sub_id
        and isinstance(questions[0].get(sub_id), list)
    ):
        return questions[0][sub_id], True
    return questions, False


def load_all_question_locations():
    """
    Returns:
      file_data_map: rel_file -> parsed json data
      locations_by_key: (topic_id, subcategory_id, question_id, norm_question) -> location dict
      norms_by_topic: topic_id -> set(norm_question)
    """
    file_data_map = {}
    locations_by_key = {}
    norms_by_topic = defaultdict(set)

    for topic_info in collect_topic_files():
        rel_file = topic_info["file"]
        abs_file = ROOT / rel_file
        if not abs_file.exists():
            continue
        data = load_json(abs_file)
        file_data_map[rel_file] = data
        subcategories = data.get("subcategories", []) if isinstance(data, dict) else []
        for sub in subcategories:
            if not isinstance(sub, dict):
                continue
            sub_id = sub.get("id", "")
            q_list, nested = iterate_subcategory_questions(sub)
            for idx, q in enumerate(q_list):
                if not isinstance(q, dict):
                    continue
                qid = str(q.get("id", "")).strip()
                qtext = str(q.get("question", "")).strip()
                norm = normalize_text(qtext)
                key = (topic_info["topic_id"], sub_id, qid, norm)
                locations_by_key[key] = {
                    "topic_id": topic_info["topic_id"],
                    "subcategory_id": sub_id,
                    "question_id": qid,
                    "question": qtext,
                    "norm_question": norm,
                    "source_file": rel_file,
                    "index": idx,
                    "nested": nested,
                }
                if norm:
                    norms_by_topic[topic_info["topic_id"]].add(norm)

    return file_data_map, locations_by_key, norms_by_topic


def build_safe_relevance_removals(audit_report: dict, norms_by_topic: Dict[str, set]):
    candidates = []
    for flag in audit_report.get("relevance_flags", []):
        q = flag.get("question", {})
        reasons = flag.get("reasons", [])
        best_other_topic = flag.get("best_other_topic")
        own_topic_score = flag.get("own_topic_score", 0)
        own_sub_score = flag.get("own_subcategory_score", 0)
        best_other_score = flag.get("best_other_score", 0)
        question_text = q.get("question", "")
        norm = normalize_text(question_text)

        looks_closer = any(str(r).startswith("looks_closer_to:") for r in reasons)
        high_conf = (
            looks_closer
            and own_topic_score == 0
            and own_sub_score == 0
            and best_other_score >= 4
            and best_other_topic
        )
        if not high_conf or not norm:
            continue

        if norm in norms_by_topic.get(best_other_topic, set()):
            candidates.append(
                {
                    "topic_id": q.get("topic_id", ""),
                    "subcategory_id": q.get("subcategory_id", ""),
                    "question_id": q.get("question_id", ""),
                    "question": question_text,
                    "norm_question": norm,
                    "source_file": q.get("source_file", ""),
                    "best_other_topic": best_other_topic,
                    "best_other_score": best_other_score,
                }
            )

    # Deduplicate exact same item references.
    seen = set()
    unique = []
    for c in candidates:
        k = (c["topic_id"], c["subcategory_id"], c["question_id"], c["source_file"])
        if k in seen:
            continue
        seen.add(k)
        unique.append(c)
    return unique


def apply_removals(file_data_map: Dict[str, dict], removals: List[dict], locations_by_key: Dict[tuple, dict]):
    remove_map = defaultdict(list)  # (source_file, subcategory_id, nested) -> [indices]

    for r in removals:
        key = (r["topic_id"], r["subcategory_id"], r["question_id"], r["norm_question"])
        loc = locations_by_key.get(key)
        if not loc:
            continue
        remove_map[(loc["source_file"], loc["subcategory_id"], loc["nested"])].append(loc["index"])

    changed_files = set()
    for (source_file, sub_id, nested), indices in remove_map.items():
        data = file_data_map[source_file]
        subcategories = data.get("subcategories", [])
        for sub in subcategories:
            if not isinstance(sub, dict) or sub.get("id") != sub_id:
                continue
            q_list, is_nested = iterate_subcategory_questions(sub)
            if is_nested != nested:
                continue
            to_remove = set(indices)
            filtered = [q for i, q in enumerate(q_list) if i not in to_remove]
            if nested:
                sub["questions"][0][sub_id] = filtered
            else:
                sub["questions"] = filtered
            changed_files.add(source_file)
            break

    for rel in changed_files:
        dump_json(ROOT / rel, file_data_map[rel])

    return sorted(changed_files)


def write_markdown(report: dict, path: Path):
    lines = []
    lines.append("# Safe Relevance Cleanup Proposal")
    lines.append("")
    lines.append("## Summary")
    lines.append(
        f"- High-confidence removable mismatches (target-topic copy exists): **{report['summary']['to_remove_count']}**"
    )
    lines.append(f"- Files changed: **{report['summary']['files_changed']}**")
    lines.append(f"- Applied: **{report['summary']['applied']}**")
    lines.append("")

    lines.append("## Removal List")
    if not report["removals"]:
        lines.append("- None")
    else:
        for item in report["removals"]:
            lines.append(
                f"- `{item['question_id']}` [{item['topic_id']}/{item['subcategory_id']}] in `{item['source_file']}` "
                f"-> closer to `{item['best_other_topic']}`"
            )
            lines.append(
                f"  - {item['question'][:180]}{'...' if len(item['question']) > 180 else ''}"
            )

    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def main():
    parser = argparse.ArgumentParser(description="Safe relevance cleanup")
    parser.add_argument("--apply", action="store_true", help="Write removals to data files")
    parser.add_argument("--audit-json", default=str(AUDIT_JSON))
    parser.add_argument("--json-out", default=str(DEFAULT_JSON_OUT))
    parser.add_argument("--md-out", default=str(DEFAULT_MD_OUT))
    args = parser.parse_args()

    audit_report = load_json(Path(args.audit_json))
    file_data_map, locations_by_key, norms_by_topic = load_all_question_locations()

    removals = build_safe_relevance_removals(audit_report, norms_by_topic)
    changed_files = []
    if args.apply and removals:
        changed_files = apply_removals(file_data_map, removals, locations_by_key)

    report = {
        "summary": {
            "to_remove_count": len(removals),
            "files_changed": len(changed_files),
            "applied": bool(args.apply),
        },
        "changed_files": changed_files,
        "removals": removals,
    }

    json_out = Path(args.json_out)
    md_out = Path(args.md_out)
    json_out.parent.mkdir(parents=True, exist_ok=True)
    json_out.write_text(json.dumps(report, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
    write_markdown(report, md_out)

    print("Safe relevance cleanup complete")
    print(json.dumps(report["summary"], indent=2))
    print(f"JSON report: {json_out}")
    print(f"Markdown report: {md_out}")


if __name__ == "__main__":
    main()
